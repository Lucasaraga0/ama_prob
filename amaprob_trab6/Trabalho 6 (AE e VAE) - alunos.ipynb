{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import v2\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "batch_size = 512\n",
    "learning_rate = 5*1e-3\n",
    "num_epochs = 50\n",
    "latent_dim = 2\n",
    "hidden_dim = [512, 256]\n",
    "\n",
    "transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Lambda(lambda x: x.view(-1)),\n",
    "])\n",
    "\n",
    "train_data = datasets.FashionMNIST('FashionMNIST_data/', download=False, train=True, transform=transform)\n",
    "test_data = datasets.FashionMNIST('FashionMNIST_data/', download=False, train=False, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for X, _ in train_loader:\n",
    "    input_dim = X.shape[1]\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    break\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AEOutput:\n",
    "    \"\"\"\n",
    "    Dataclass for AE output.\n",
    "    \"\"\"\n",
    "    z_proj: torch.Tensor    \n",
    "    x_recon: torch.Tensor    \n",
    "    loss: torch.Tensor\n",
    "\n",
    "class AE(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoencoder (VAE) class.\n",
    "    \n",
    "    Args:\n",
    "        input_dim (int): Dimensionality of the input data.\n",
    "        hidden_dim (int): Dimensionality of the hidden layers.\n",
    "        latent_dim (int): Dimensionality of the latent space.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(AE, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        encoder_layers_list = []\n",
    "        if type(hidden_dim) != list:\n",
    "            encoder_layers_list.append(nn.Linear(input_dim, hidden_dim))\n",
    "            encoder_layers_list.append(nn.LeakyReLU())\n",
    "            encoder_layers_list.append(nn.Linear(hidden_dim, 2 * latent_dim))\n",
    "        else:\n",
    "            self.hidden_dim = hidden_dim.copy()\n",
    "            encoder_layers_list.append(nn.Linear(input_dim, hidden_dim[0]))\n",
    "            encoder_layers_list.append(nn.LeakyReLU())\n",
    "            for i in range(len(hidden_dim[1:])):\n",
    "                encoder_layers_list.append(nn.Linear(hidden_dim[i], hidden_dim[i+1]))\n",
    "                encoder_layers_list.append(nn.LeakyReLU())\n",
    "            encoder_layers_list.append(nn.Linear(hidden_dim[-1], latent_dim))\n",
    "        \n",
    "        self.encoder = nn.Sequential(*encoder_layers_list)\n",
    "\n",
    "        decoder_layers_list = []\n",
    "        if type(hidden_dim) != list:\n",
    "            decoder_layers_list.append(nn.Linear(latent_dim, hidden_dim))\n",
    "            decoder_layers_list.append(nn.LeakyReLU())\n",
    "            decoder_layers_list.append(nn.Linear(hidden_dim, input_dim))\n",
    "            decoder_layers_list.append(nn.Sigmoid())\n",
    "        else:\n",
    "            hidden_dim_decoder = hidden_dim[::-1]\n",
    "            decoder_layers_list.append(nn.Linear(latent_dim, hidden_dim_decoder[0]))\n",
    "            decoder_layers_list.append(nn.LeakyReLU())\n",
    "            for i in range(len(hidden_dim_decoder[1:])):\n",
    "                decoder_layers_list.append(nn.Linear(hidden_dim_decoder[i], hidden_dim_decoder[i+1]))\n",
    "                decoder_layers_list.append(nn.LeakyReLU())\n",
    "            decoder_layers_list.append(nn.Linear(hidden_dim_decoder[-1], input_dim))\n",
    "            decoder_layers_list.append(nn.Sigmoid())\n",
    "\n",
    "        self.decoder = nn.Sequential(*decoder_layers_list)\n",
    "\n",
    "        print(f\"Encoder: {encoder_layers_list}\")\n",
    "        print(f\"Decoder: {decoder_layers_list}\")\n",
    "\n",
    "    def encode(self, x):        \n",
    "        return self.encoder(x)\n",
    "        \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x, compute_loss: bool = True):\n",
    "        z_proj = self.encode(x)\n",
    "        recon_x = self.decode(z_proj)\n",
    "        \n",
    "        if not compute_loss:\n",
    "            return AEOutput(\n",
    "                z_proj=z_proj,\n",
    "                x_recon=recon_x,\n",
    "                loss=None,\n",
    "            )\n",
    "        \n",
    "        loss = F.binary_cross_entropy(recon_x, x, reduction='none').sum(-1).mean()\n",
    "        \n",
    "        return AEOutput(\n",
    "                z_proj=z_proj,\n",
    "                x_recon=recon_x,\n",
    "                loss=loss,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoder definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class VAEOutput:\n",
    "    \"\"\"\n",
    "    Dataclass for VAE output.\n",
    "    \n",
    "    Attributes:\n",
    "        z_dist (torch.distributions.Distribution): The distribution of the latent variable z.\n",
    "        z_sample (torch.Tensor): The sampled value of the latent variable z.\n",
    "        x_recon (torch.Tensor): The reconstructed output from the VAE.\n",
    "        loss (torch.Tensor): The overall loss of the VAE.\n",
    "        loss_recon (torch.Tensor): The reconstruction loss component of the VAE loss.\n",
    "        loss_kl (torch.Tensor): The KL divergence component of the VAE loss.\n",
    "    \"\"\"\n",
    "    z_dist: torch.distributions.Distribution\n",
    "    z_sample: torch.Tensor\n",
    "    x_recon: torch.Tensor\n",
    "    \n",
    "    loss: torch.Tensor\n",
    "    loss_recon: torch.Tensor\n",
    "    loss_kl: torch.Tensor\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Variational Autoencoder (VAE) class.\n",
    "    \n",
    "    Args:\n",
    "        input_dim (int): Dimensionality of the input data.\n",
    "        hidden_dim (int): Dimensionality of the hidden layers.\n",
    "        latent_dim (int): Dimensionality of the latent space.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        encoder_layers_list = []\n",
    "        if type(hidden_dim) != list:\n",
    "            encoder_layers_list.append(nn.Linear(input_dim, hidden_dim))\n",
    "            encoder_layers_list.append(nn.LeakyReLU())\n",
    "            encoder_layers_list.append(nn.Linear(hidden_dim, 2 * latent_dim))\n",
    "        else:\n",
    "            self.hidden_dim = hidden_dim.copy()\n",
    "            encoder_layers_list.append(nn.Linear(input_dim, hidden_dim[0]))\n",
    "            encoder_layers_list.append(nn.LeakyReLU())\n",
    "            for i in range(len(hidden_dim[1:])):\n",
    "                encoder_layers_list.append(nn.Linear(hidden_dim[i], hidden_dim[i+1]))\n",
    "                encoder_layers_list.append(nn.LeakyReLU())\n",
    "            encoder_layers_list.append(nn.Linear(hidden_dim[-1], 2 * latent_dim))\n",
    "        \n",
    "        self.encoder = nn.Sequential(*encoder_layers_list)\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "        decoder_layers_list = []\n",
    "        if type(hidden_dim) != list:\n",
    "            decoder_layers_list.append(nn.Linear(latent_dim, hidden_dim))\n",
    "            decoder_layers_list.append(nn.LeakyReLU())\n",
    "            decoder_layers_list.append(nn.Linear(hidden_dim, input_dim))\n",
    "            decoder_layers_list.append(nn.Sigmoid())\n",
    "        else:\n",
    "            hidden_dim_decoder = hidden_dim[::-1]\n",
    "            decoder_layers_list.append(nn.Linear(latent_dim, hidden_dim_decoder[0]))\n",
    "            decoder_layers_list.append(nn.LeakyReLU())\n",
    "            for i in range(len(hidden_dim_decoder[1:])):\n",
    "                decoder_layers_list.append(nn.Linear(hidden_dim_decoder[i], hidden_dim_decoder[i+1]))\n",
    "                decoder_layers_list.append(nn.LeakyReLU())\n",
    "            decoder_layers_list.append(nn.Linear(hidden_dim_decoder[-1], input_dim))\n",
    "            decoder_layers_list.append(nn.Sigmoid())\n",
    "\n",
    "        self.decoder = nn.Sequential(*decoder_layers_list)\n",
    "\n",
    "        print(f\"Encoder: {encoder_layers_list}\")\n",
    "        print(f\"Decoder: {decoder_layers_list}\")\n",
    "\n",
    "    def encode(self, x, eps: float = 1e-8):\n",
    "        \"\"\"\n",
    "        Encodes the input data into the latent space.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input data.\n",
    "            eps (float): Small value to avoid numerical instability.\n",
    "        \n",
    "        Returns:\n",
    "            torch.distributions.MultivariateNormal: Normal distribution of the encoded data.\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "            COMPLETE AQUI!\n",
    "            \n",
    "            Codifique a entrada em uma distribuição variacional Gaussiana.\n",
    "            Lembre que o codificador retorna as médias e os logs das variâncias.\n",
    "            Você pode usar eps para evitar uma variância excessivamente pequena.\n",
    "            Você deve retornar um objeto torch.distributions.MultivariateNormal.\n",
    "            \n",
    "            COMPLETE AQUI!\n",
    "        \"\"\"\n",
    "        \n",
    "    def reparameterize(self, dist):\n",
    "        \"\"\"\n",
    "        Reparameterizes the encoded data to sample from the latent space.\n",
    "        \n",
    "        Args:\n",
    "            dist (torch.distributions.MultivariateNormal): Normal distribution of the encoded data.\n",
    "        Returns:\n",
    "            torch.Tensor: Sampled data from the latent space.\n",
    "        \"\"\"\n",
    "        return dist.rsample()\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        Decodes the data from the latent space to the original input space.\n",
    "        \n",
    "        Args:\n",
    "            z (torch.Tensor): Data in the latent space.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Reconstructed data in the original input space.\n",
    "        \"\"\"\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x, compute_loss: bool = True):\n",
    "        \"\"\"\n",
    "        Performs a forward pass of the VAE.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input data.\n",
    "            compute_loss (bool): Whether to compute the loss or not.\n",
    "        \n",
    "        Returns:\n",
    "            VAEOutput: VAE output dataclass.\n",
    "        \"\"\"\n",
    "        \n",
    "        dist = self.encode(x)\n",
    "        z = self.reparameterize(dist)\n",
    "        recon_x = self.decode(z)\n",
    "        \n",
    "        if not compute_loss:\n",
    "            return VAEOutput(\n",
    "                z_dist=dist,\n",
    "                z_sample=z,\n",
    "                x_recon=recon_x,\n",
    "                loss=None,\n",
    "                loss_recon=None,\n",
    "                loss_kl=None,\n",
    "            )\n",
    "        \n",
    "        # compute loss terms \n",
    "        loss_recon = F.binary_cross_entropy(recon_x, x, reduction='none').sum(-1).mean()\n",
    "        std_normal = torch.distributions.MultivariateNormal(\n",
    "            torch.zeros_like(z, device=z.device),\n",
    "            scale_tril=torch.eye(z.shape[-1], device=z.device).unsqueeze(0).expand(z.shape[0], -1, -1),\n",
    "        )\n",
    "        loss_kl = torch.distributions.kl.kl_divergence(dist, std_normal).mean()\n",
    "                \n",
    "        loss = loss_recon + loss_kl\n",
    "        \n",
    "        return VAEOutput(\n",
    "            z_dist=dist,\n",
    "            z_sample=z,\n",
    "            x_recon=recon_x,\n",
    "            loss=loss,\n",
    "            loss_recon=loss_recon,\n",
    "            loss_kl=loss_kl,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, prev_updates):\n",
    "    \"\"\"\n",
    "    Trains the model on the given data.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model to train.\n",
    "        dataloader (torch.utils.data.DataLoader): The data loader.\n",
    "        loss_fn: The loss function.\n",
    "        optimizer: The optimizer.\n",
    "    \"\"\"\n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    loss_history = []\n",
    "    for batch_idx, (data, _) in enumerate(tqdm(dataloader, desc='Training')):\n",
    "        n_upd = prev_updates + batch_idx\n",
    "        \n",
    "        data = data.to(device)\n",
    "       \n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        \n",
    "        output = model(data)  # Forward pass\n",
    "        \n",
    "        loss = output.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()  # Update the model parameters\n",
    "        \n",
    "        loss_history.append(loss.item())\n",
    "\n",
    "    print(f'Step {n_upd:,} (N samples: {n_upd*batch_size:,}), Train set loss: {np.mean(loss_history):.4f}') \n",
    "\n",
    "    return prev_updates + len(dataloader), loss_history\n",
    "\n",
    "def test(model, dataloader):\n",
    "    \"\"\"\n",
    "    Tests the model on the given data.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model to test.\n",
    "        dataloader (torch.utils.data.DataLoader): The data loader.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, _ in tqdm(dataloader, desc='Testing'):\n",
    "            data = data.to(device)\n",
    "            data = data.view(data.size(0), -1)  # Flatten the data\n",
    "            \n",
    "            output = model(data, compute_loss=True)  # Forward pass\n",
    "            \n",
    "            test_loss += output.loss.item()\n",
    "            \n",
    "    test_loss /= len(dataloader)\n",
    "    print(f'====> Test set loss: {test_loss:.4f}')\n",
    "\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Autoencoder...\")\n",
    "model_AE = AE(input_dim=input_dim, hidden_dim=hidden_dim, latent_dim=latent_dim).to(device)\n",
    "print(model_AE)\n",
    "optimizer = torch.optim.AdamW(model_AE.parameters(), lr=learning_rate)\n",
    "\n",
    "train_loss_history_AE = []\n",
    "test_loss_history_AE = []\n",
    "prev_updates = 0\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    prev_updates, train_loss = train(model_AE, train_loader, optimizer, prev_updates)\n",
    "    test_loss = test(model_AE, test_loader)\n",
    "    train_loss_history_AE.append(np.mean(train_loss))\n",
    "    test_loss_history_AE.append(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,len(train_loss_history_AE)+1), train_loss_history_AE, label=\"Train loss (epoch average)\")\n",
    "plt.plot(range(1,len(train_loss_history_AE)+1), test_loss_history_AE, label=\"Test loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"Autoencoder\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Variational Autoencoder...\")\n",
    "model_VAE = VAE(input_dim=input_dim, hidden_dim=hidden_dim, latent_dim=latent_dim).to(device)\n",
    "print(model_VAE)\n",
    "optimizer = torch.optim.AdamW(model_VAE.parameters(), lr=learning_rate)\n",
    "\n",
    "train_loss_history_VAE = []\n",
    "test_loss_history_VAE = []\n",
    "prev_updates = 0\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    prev_updates, train_loss = train(model_VAE, train_loader, optimizer, prev_updates)\n",
    "    test_loss = test(model_VAE, test_loader)\n",
    "    train_loss_history_VAE.append(np.mean(train_loss))\n",
    "    test_loss_history_VAE.append(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,len(train_loss_history_VAE)+1), train_loss_history_VAE, label=\"Train loss (epoch average)\")\n",
    "plt.plot(range(1,len(train_loss_history_VAE)+1), test_loss_history_VAE, label=\"Test loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"Variational Autoencoder\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent_space(model):\n",
    "    model.eval()\n",
    "    z_all = []\n",
    "    y_all = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(train_loader, desc='Encoding'):\n",
    "            \"\"\"\n",
    "            COMPLETE AQUI!\n",
    "            \n",
    "            Projete os dados de treinamento.\n",
    "            \n",
    "            COMPLETE AQUI!\n",
    "            \"\"\"\n",
    "            y_all.append(target.numpy())\n",
    "\n",
    "    z_all = np.concatenate(z_all, axis=0)\n",
    "    y_all = np.concatenate(y_all, axis=0)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.scatter(z_all[:, 0], z_all[:, 1], c=y_all, cmap='tab10')\n",
    "    plt.xlim(-6,6)\n",
    "    plt.ylim(-6,6)\n",
    "    plt.colorbar()\n",
    "    plt.title(f'Latent projection')\n",
    "    plt.show()\n",
    "\n",
    "def plot_random_samples(model):\n",
    "    \"\"\"\n",
    "    COMPLETE AQUI!\n",
    "    \n",
    "    Gere 100 novas amostras na variável 'samples'\n",
    "    \n",
    "    COMPLETE AQUI!\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(10, 10, figsize=(10, 10))\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            ax[i, j].imshow(samples[i*10+j].view(28, 28).cpu().detach().numpy(), cmap='gray')\n",
    "            ax[i, j].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Autoencoder:\")\n",
    "plot_latent_space(model_AE)\n",
    "print(\"Variational Autoencoder:\")\n",
    "plot_latent_space(model_VAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Autoencoder:\")\n",
    "plot_random_samples(model_AE)\n",
    "print(\"Variational Autoencoder:\")\n",
    "plot_random_samples(model_VAE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
