{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a84866d",
   "metadata": {},
   "source": [
    "## Lista 2 - Aprendizagem de máquina probabilístico\n",
    "- Aluno: Lucas Rodrigues Aragão - Graduação 538390"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28361a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea45c25c",
   "metadata": {},
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0cc668",
   "metadata": {},
   "source": [
    "### Regressão linear bayesiana\n",
    "\n",
    "#### Estimação \n",
    "1. Definir, a partir de conhecimentos anteriores, momentos da priori $p(w) = \\mathcal{N}(w|m_0,S_0)$ e a variância do ruído, $p(\\epsilon) = \\mathcal{N}(\\epsilon|0, \\sigma^2)$\n",
    "\n",
    "2. A partir de $\\mathcal{D}=(X,y)$, calcular a posteriori de w\n",
    "    - $P(w|D) = \\mathcal{N}(w|\\mu,\\Sigma)$, em que\n",
    "    - $\\mu = m_0 + (S_0 X^T+ \\sigma^2I)^{-1}S_0X^T(y-X m_0)$ \n",
    "    - $\\Sigma = S_0 - (S_0 X^T X + \\sigma^2 I)^{-1} S_0 X^T X S_0$\n",
    "3. Retornar a posteriori dos parâmetros $p(w|D)$\n",
    "\n",
    "#### Predição\n",
    "1. A partir dos dados $X_{\\ast} \\in \\mathbb{R}^{(N_{\\ast} \\times D)}$, retornar a distribuição \n",
    "\n",
    "$$p(y_{\\ast}| X_{\\ast}) = \\mathcal{N}(y_{\\ast}|X_{\\ast}\\mu, X_{\\ast} \\Sigma X_{\\ast}^T + \\sigma^2I)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cf6abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: ajeitar os valores dos hiperparametros iniciais\n",
    "class BayesianLinReg():\n",
    "    def __init__(self, sigma2,m0,s0):\n",
    "        self.sigma2 = sigma2\n",
    "        self.m0 = m0\n",
    "        self.s0 = s0 \n",
    "        \n",
    "    def estimate(self, X, y):\n",
    "\n",
    "        A_inv = np.linalg.inv(self.s0 @ X.T @ X + self.sigma2 * np.eye(X.shape[0])) # termo comum nas duas contas\n",
    "        self.mu = self.m0 + A_inv @ self.s0 @ X.T @ (y - X @ self.m0)\n",
    "        self.Sigma = self.s0 - A_inv @ self.s0 @ X.T @ X @ self.s0\n",
    "        \n",
    "        posteriori  = np.random.multivariate_normal(self.mu.flatten(), self.Sigma) # N(w|self.mi, self.sigma)\n",
    "        return posteriori\n",
    "    \n",
    "    def predict(self, X_ast):\n",
    "        media = X_ast @ self.mu \n",
    "        var = X_ast @ self.Sigma @ X_ast.T + (self.sigma2 * np.eye(X_ast.shape[0]))\n",
    "\n",
    "        predictions = np.random.multivariate_normal(media, var)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7558f996",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression_data = pd.read_csv(\"linear_regression_data.csv\")\n",
    "X = linear_regression_data[\"col1\"]\n",
    "y = linear_regression_data[\"col2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7d378f",
   "metadata": {},
   "source": [
    "### Regressão Polinomial Bayesiana\n",
    "\n",
    "As expressões do modelo continuam as mesmas.\n",
    "As mudanças são que, agora temos,\n",
    "\n",
    "$$\\Phi = \\phi(X) = [\\phi(x_1), \\phi(x_2), \\cdots, \\phi(x_N)]^T$$\n",
    "em que $\\phi(x_i) = [1,x_i, x_i^2, \\cdots, x_i^P]^T$, onde $P$ é a ordem do polinômio desejado\n",
    "\n",
    "E com isso nosso $y$ será definido por $y= \\Phi w + \\epsilon$. Dessa forma o cálculo de $\\mu$ e $\\Sigma$ seram dados por \n",
    "\n",
    "- $\\mu = m_0 +(S_0 \\Phi^T \\Phi + \\sigma^2I)^{-1}S_0 \\Phi^T (y - \\Phi m_0) $  \n",
    "- $\\Sigma = S_0 +(S_0 \\Phi^T \\Phi + \\sigma^2I)^{-1}S_0 \\Phi^T \\Phi S_0$\n",
    "\n",
    "E a nossa predição é dada por \n",
    "\n",
    "$$p(y_\\ast| X_\\ast, \\mathcal{D}, m_0, S_0, \\sigma^2) = \\mathcal{N}(y_\\ast|\\Phi_\\ast \\mu , \\Phi_\\ast \\Sigma \\Phi_\\ast^T + \\sigma^2I)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfe26cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianPolyReg():\n",
    "    def __init__(self, sigma2, m0, s0, degree):\n",
    "        self.sigma2 = sigma2\n",
    "        self.m0 = m0\n",
    "        self.s0 = s0\n",
    "        self.degree = degree\n",
    "    \n",
    "    def elevate(self, X: pd.DataFrame):\n",
    "        x_vals = X.values.flatten()  \n",
    "        Phi = np.vstack([x_vals**d for d in range(self.degree + 1)]).T        \n",
    "        return Phi\n",
    "\n",
    "    def estimate(self, X,y):\n",
    "        Phi = elevate(X)\n",
    "        A_inv = np.linalg.inv(self.s0 @ Phi.T @ Phi + self.sigma2 * np.eye(Phi.shape[0])) # termo comum nas duas contas\n",
    "        self.mu = self.m0 + A_inv @ self.s0 @ X.T @ (y - Phi @ self.m0)\n",
    "        self.Sigma = self.s0 - A_inv @ self.s0 @ Phi.T @ Phi @ self.s0\n",
    "        \n",
    "        posteriori  = np.random.multivariate_normal(self.mu.flatten(), self.Sigma) # N(w|self.mi, self.sigma)\n",
    "        return posteriori\n",
    "    \n",
    "    def predict(self, X_ast):\n",
    "        # fazer a transformacao do X_ast em Phi_ast\n",
    "        Phi_ast = elevate(X_ast)\n",
    "        media = Phi_ast @ self.mu \n",
    "        var = Phi_ast @ self.Sigma @ Phi_ast.T + (self.sigma2 * np.eye(Phi_ast.shape[0]))\n",
    "\n",
    "        predictions = np.random.multivariate_normal(media, var)\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e399883b",
   "metadata": {},
   "source": [
    "### Regressão logística IRLS\n",
    "\n",
    "#### Estimação\n",
    "1. Definir, a partir de conhecimentos anteriores, os momentos da priori $p(w) = \\mathcal{N}(w|m_0,S_0)$ e o valor inicial de $m_0 \\in \\mathbb{R}^D$.\n",
    "\n",
    "2. A partir dos dados repetir até convergência:\n",
    "    $$w_t = w_{t-1} + A^{-1}\\big[X^T(y - \\sigma(Xw_{t-1})) - S_0^{-1}(w_{t-1} - m_0)]$$\n",
    "    - Em que\n",
    "        - $A =X^TR_{t-1}X + S_0^{-1}$\n",
    "        - $R_{t-1} = \\text{diag}([R_{t-1}]_{11}, \\cdots, [R_{t-1}]_{NN})$\n",
    "        - $[R]_{ii} = \\sigma(w^T_{t-1} x_i) (1 - \\sigma(w^T_{t-1} x_i))$\n",
    "\n",
    "3. Retornar os parâmetros $\\hat{w}$\n",
    "\n",
    "#### Predição \n",
    "\n",
    "1. Dado $x_{\\ast}$, a predição é dada por:\n",
    "    $$p(y_{\\ast} = 1| x_\\ast) = \\sigma(\\hat{w}^Tx_{\\ast})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810fdffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoide(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "class LogisIRLS():\n",
    "    def __init__(self, m0, s0, w0):\n",
    "        self.m0 = m0\n",
    "        self.s0 = s0\n",
    "        self.W = w0\n",
    "\n",
    "    def estimate(self, X:pd.DataFrame, y, epochs, conv_lim):\n",
    "        i = 0\n",
    "        dif = np.inf()\n",
    "        n = X.shape[0]    \n",
    "        inv_s0 = np.linalg.inv(self.s0)\n",
    "        R = np.zeros(shape= (n,n))\n",
    "\n",
    "        while (i<epochs or dif< conv_lim):\n",
    "            # preencher a matriz R \n",
    "            old_w = self.W\n",
    "            for index, row in X.iterrows():\n",
    "                sigm = sigmoide(old_w.T @ row)\n",
    "                R[index][index] = sigm  * (1 - sigm)    \n",
    "\n",
    "            # calcular a matriz A\n",
    "            A = X.T @ R @ X + inv_s0\n",
    "            # atualizar os pesos \n",
    "            next_w = old_w + np.linalg.inv(A) @ (X.T @ (y- sigmoide(X @ old_w))) - inv_s0 @ (old_w- self.m0)\n",
    "            self.W = next_w\n",
    "\n",
    "            # calcular criterios de parada\n",
    "            dif = np.abs(old_w - next_w)\n",
    "            i += 1\n",
    "\n",
    "        \n",
    "\n",
    "    def predict(self, X_ast):\n",
    "        return sigmoide(self.W.T @ X_ast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f328ae72",
   "metadata": {},
   "source": [
    "### Regressão logística Bayesiana\n",
    "\n",
    "#### Estimação \n",
    "- Passos 1 a 3 do IRLS para encontrar o $\\hat{w}$\n",
    "\n",
    "- Aproximar a posteriori de $w$\n",
    "\n",
    "$$p(w|\\mathcal{D}) \\approx \\mathcal{N}(w|\\hat{w}, H^{-1})$$\n",
    "\n",
    "- Em que \n",
    "    - $H = X^T\\hat{R}X + S_0^{-1}$\n",
    "    - $\\hat{R} = \\text{diag} (\\hat{R}_{11}, \\cdots, \\hat{R}_{NN})$\n",
    "    - $\\hat{R}_{ii} = \\sigma(\\hat{w}^T x_i)(1- \\sigma(\\hat{w}^T x_i))$\n",
    "\n",
    "#### Predição \n",
    "\n",
    "1. Dado $x_{\\ast}$, retornar a distribuição preditiva\n",
    "\n",
    "- Usando **Monte Carlo**: \n",
    "$$p(y_{\\ast}|x_{\\ast} \\approx \\frac{1}{S} \\sum^S_{s=1} \\sigma(w_s^Tx_{\\ast}))$$\n",
    "\n",
    "- Onde $w_s \\sim \\mathcal{N}(w|\\hat{w}, H^{-1})$\n",
    "\n",
    "- Usando **probit**\n",
    "\n",
    "$$p(y_\\ast = 1|x_\\ast) \\approx \\sigma((1+ \\pi \\sigma^w_a/8)^{1/2} \\mu_a)$$\n",
    "\n",
    "- Onde $\\mu_a = w^T x_\\ast$ e $\\sigma^2_a = x_\\ast^T H^{-1}x_\\ast$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0753ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianRegLogis():\n",
    "    def __init__(self, m0, s0, w0):\n",
    "        self.m0= m0\n",
    "        self.s0 = s0\n",
    "        self.W = w0\n",
    "    \n",
    "    def estimate(self, X:pd.DataFrame, y, epochs, conv_lim):\n",
    "        irls = LogisIRLS(m0= self.m0, s0= self.s0,w0= self.W)\n",
    "        irls.estimate(X = X, y = y, epochs=epochs, conv_lim= conv_lim)\n",
    "        self.W = irls.W\n",
    "        w_hat = self.W\n",
    "        n = X.shape[0]\n",
    "        R_hat = np.zeros((n,n))\n",
    "        \n",
    "        for index, row in X.iterrows():\n",
    "            sigm = sigmoide(w_hat.T @ row)\n",
    "            R_hat[index][index] = sigm  * (1 - sigm)    \n",
    "        \n",
    "        H = X @ R_hat @ X + np.linalg.inv(self.s0)\n",
    "        self.H = H\n",
    "\n",
    "        #aproximar a  posteriori de w\n",
    "        posteriori = np.random.multivariate_normal(w_hat, np.linalg.inv(H))\n",
    "        self.posteriori_hat = posteriori\n",
    "\n",
    "    def predict(self, X_ast, approx = \"mc\"):\n",
    "        if approx == \"mc\":\n",
    "            # TODO: Implementar a aproximacao de monte carlos\n",
    "            #aproximacao de monte carlo\n",
    "            pass\n",
    "        else:\n",
    "            #aproximacao de probit\n",
    "            mu_a = self.posteriori_hat @ X_ast\n",
    "            sigma_a = X_ast.T @ np.linalg.inv(self.H) @ X_ast\n",
    "            pred_1 = np.sqrt(1 + np.pi()*sigma_a/8)\n",
    "            predictions = sigmoide(pred_1 * mu_a)\n",
    "        return predictions\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ce327a",
   "metadata": {},
   "source": [
    "## Questões"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aba0c8",
   "metadata": {},
   "source": [
    "### Questão 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
