{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2a9c449",
   "metadata": {},
   "source": [
    "## Lista 5 - Aprendizagem de Máquina Probabilístico\n",
    "- Aluno: Lucas Rodrigues Aragão - Graduação 538390"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def29138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.scipy.linalg import cholesky\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1846cc",
   "metadata": {},
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101d7b5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8bcf1be8",
   "metadata": {},
   "source": [
    "### Inferência com modelos de GP para regressão \n",
    "- Estimação \n",
    "\n",
    "    1. Inicializar hiperparâmetros $\\theta = \\big[  \\sigma^2_f, w^2_1, \\cdots, w^2_D, \\sigma^2_y \\big]^T$\n",
    "\n",
    "    2. Repetir até convergir ou número de épocas\n",
    "\n",
    "        1. Calcular a evidência do modelo $\\log p(y|X, \\theta)$. A evidência é calculada via Cholesky.\n",
    "            $$K+ \\sigma^2_y = LL^T, \\alpha = L^{-1}y$$\n",
    "            $$\\mathcal{L} (\\theta) = - \\sum_{i}{\\log L_{ii} - \\frac{1}{2} \\alpha^T \\alpha - \\frac{N}{2} \\log (2 \\pi)}$$\n",
    "        2. Calcular os gradientes analíticos $\\frac{\\partial \\log p(y|X, \\theta)}{\\partial \\theta}$.\n",
    "        3. Atualizar $\\theta$ a partir dos gradientes.\n",
    "\n",
    "    3. Retornar os hiperparâmetros\n",
    "\n",
    "- Predição\n",
    "\n",
    "1. Dado um novo padrão $x_\\ast$ retornar a distribuição preditiva \n",
    "\n",
    "$$p(y| x_\\ast, y, X, \\hat{\\theta}) = \\mathcal{N}(y_\\ast| \\mu_\\ast, \\sigma^2_\\ast + \\sigma^2_y)$$\n",
    "\n",
    "$$\\mu_\\ast = k_{f \\ast}^T (K + \\sigma^2_y I)^{-1} y$$\n",
    "\n",
    "$$\\sigma^2_\\ast = k_{\\ast \\ast} - k_{f \\ast}^T (K + \\sigma^2_y I)^{-1} k_{f \\ast}$$\n",
    "\n",
    "Em que, $k_{f \\ast} = [k(x_\\ast, x_1), \\cdots , k(x_\\ast, x_N)]$ e $k_{\\ast \\ast} = k(x_\\ast, x_\\ast)$\n",
    "\n",
    "Além disso, valores de inicialização comuns são, $\\sigma^2_f = \\mathbb{V}[y]$, $w^2_d = \\frac{1}{\\mathbb{V}[X_{:d}]}$ e $\\sigma^2_y = 0.01 \\sigma^2_f$, com o $\\mathbb{V}$ sendo a variância.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b76c229",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegGP:\n",
    "    def __init__(self, sigma2f, w0, sigma2y):\n",
    "\n",
    "        self.theta = jnp.array([sigma2f, w0, sigma2y])\n",
    "\n",
    "    def RBF(self, theta, xi, xj):\n",
    "        sigma2f = theta[0]\n",
    "        diff = xi-xj\n",
    "        w = theta[1:-1] \n",
    "        temp = jnp.sum(w * diff**2)\n",
    "        return sigma2f * jnp.exp(-0.5 * temp)\n",
    "        \n",
    "    def apply_kernel(self, X, theta):\n",
    "        return jax.vmap(lambda xi: jax.vmap(lambda xj: self.RBF(theta , xi, xj))(X))(X)\n",
    "\n",
    "\n",
    "    def estimate(self, X_train, y_train ,epochs, lr):\n",
    "\n",
    "        grad_evidence = jax.grad(self.evidence)\n",
    "        theta = self.theta\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            #calcular a evidencia log P(y|X,theta), via cholesky            \n",
    "            #calcular os gradientes \n",
    "            # atualizar os hiperparametros\n",
    "            grads = grad_evidence(theta=theta,X=X_train, y= y_train)\n",
    "            theta = theta + lr*grads\n",
    "            \n",
    "        self.theta = theta\n",
    "        return theta\n",
    "    \n",
    "    def evidence(self, theta, X ,y):\n",
    "        K = self.apply_kernel(theta, X)\n",
    "        sigma2y = theta[-1]\n",
    "        N = y.shape[0]\n",
    "        L = cholesky(K + sigma2y * jnp.eye(N))\n",
    "        alpha = jnp.linalg.solve(L.T, jnp.linalg.solve(L,y))\n",
    "        evidencia = - jnp.sum(jnp.log(jnp.diagonal(L))) - 0.5 * alpha.T @ alpha - 0.5 * N * jnp.log(2 * jnp.pi)\n",
    "\n",
    "        return evidencia\n",
    "\n",
    "    def predict(self, X_ast):\n",
    "        #TODO: Terminar predict\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
